# Isomorphic Math Engine

**Geometric embeddings that encode mathematical meaning, not syntax.**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## The Discovery

We trained a neural encoder that maps mathematical equations to 64-dimensional hyperbolic space. The key finding:

**Syntactically different but mathematically equivalent equations map to the same geometric region.**

This isn't pattern matching or coefficient extraction â€” the network learned genuine mathematical structure.

## Proof: Form Invariance Test

We tested whether the encoder learned to "read coefficients" or understand mathematics by generating the same quadratic equation (roots râ‚=2, râ‚‚=3) in 7 different syntactic forms:

| Form | Expression | RÂ² Score |
|------|------------|----------|
| Standard | `xÂ² - 5x + 6 = 0` | 0.998 |
| Scaled 2x | `2xÂ² - 10x + 12 = 0` | 0.996 |
| Scaled 0.5x | `0.5xÂ² - 2.5x + 3 = 0` | 0.994 |
| Negated | `-xÂ² + 5x - 6 = 0` | 0.995 |
| Rearranged | `xÂ² + 6 = 5x` | 0.998 |
| Factored tree | `xÂ·x - 2x - 3x + 6 = 0` | 0.998 |
| Negative form | `-xÂ² - bx - c = 0` | 0.998 |

**All forms predict râ‚ + râ‚‚ = 5 correctly.**

If the network were doing coefficient extraction, `2xÂ² - 10x + 12 = 0` would predict 10 (the visible coefficient), not 5 (the actual sum of roots). Instead, it understands that multiplying an equation by 2 doesn't change its solutions.

## Results Summary

| Problem Type | RÂ² Score | MAE | What's Predicted |
|--------------|----------|-----|------------------|
| Linear equations | 0.711 | 1.36 | Solution x |
| Quadratic equations | 0.999 | 0.10 | Sum of roots râ‚+râ‚‚ |
| Inequalities | 0.996 | 0.26 | Boundary value |

## Installation

```bash
pip install git+https://github.com/xfdbv99pqh-jpg/isoengine.git
```

## Quick Start

### Symbolic Solving (no training needed)

```python
from isomorphic_math import solve, parse, differentiate

# Solve equations
solve("2x + 3 = 11")           # {'x': 4.0}
solve("x^2 - 5x + 6 = 0")      # {'x': [2.0, 3.0]}
solve("2x + 3 > 7")            # {'solution': 'x > 2'}

# Parse to expression tree
expr = parse("x^2 + sin(x)")

# Symbolic differentiation
differentiate("x^3 + sin(x)")  # "3xÂ² + cos(x)"
```

### Neural Embedding (requires training)

```python
import torch
from isomorphic_math import HyperbolicEncoder, MultiHeadTrainer

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Create and train
encoder = HyperbolicEncoder().to(device)
trainer = MultiHeadTrainer(encoder, device)
trainer.train(epochs=3000)  # ~3-4 min on GPU

# Predict solutions directly from embeddings
from isomorphic_math import Eq, Add, Mul, Const, VarX

eq = Eq(Add(Mul(Const(2), VarX()), Const(3)), Const(11))  # 2x + 3 = 11
prediction = trainer.predict_linear([eq])
print(f"Predicted x = {prediction.item():.2f}")  # â‰ˆ 4.0

# Save/load trained model
trainer.save("model.pt")
trainer.load("model.pt")
```

### Similarity Detection

```python
from isomorphic_math import MathEngine

engine = MathEngine()
engine.train(epochs=2000)

# These have the same solution (x=3)
sim = engine.similarity("2x + 4 = 10", "3x - 1 = 8")
print(f"Similarity: {sim:.3f}")  # High similarity
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Expression Tree                          â”‚
â”‚         Eq(Add(Mul(Const(2), VarX()), Const(3)), Const(11)) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Tensor Encoding                            â”‚
â”‚              ops: [EQ, ADD, MUL, CONST, VAR_X, CONST, ...]  â”‚
â”‚              vals: [0, 0, 0, 2, 0, 3, ...]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Transformer Encoder                          â”‚
â”‚         4 layers, 8 heads, 256 dim â†’ 64 dim hyperbolic     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              64-dim Hyperbolic Embedding                    â”‚
â”‚     Normalized to unit sphere, encodes solution geometry    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼             â–¼             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Linear   â”‚  â”‚ Quadraticâ”‚  â”‚ Inequalityâ”‚
      â”‚ Head     â”‚  â”‚ Head     â”‚  â”‚ Head      â”‚
      â”‚ â†’ x      â”‚  â”‚ â†’ râ‚+râ‚‚  â”‚  â”‚ â†’ boundaryâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training Strategy

We use **multi-task learning** combining:

1. **Contrastive Loss**: Pulls equations with similar solutions together in embedding space
2. **Regression Loss**: Each problem type has its own head predicting the appropriate value

This teaches the embedding to both cluster equivalent equations AND encode numeric solutions.

```python
# Loss function
total_loss = contrastive_loss + 0.3 * (linear_mse + quadratic_mse + inequality_mse)
```

## Package Structure

```
isomorphic_math/
â”œâ”€â”€ __init__.py      # Exports and convenience functions
â”œâ”€â”€ core.py          # Expression system, parser, symbolic solvers
â”œâ”€â”€ encoder.py       # HyperbolicEncoder, ContrastiveTrainer
â”œâ”€â”€ multihead.py     # MultiHeadTrainer (best results)
â””â”€â”€ engine.py        # MathEngine unified API
```

## Supported Problem Types

| Type | Example | Solver | Neural Prediction |
|------|---------|--------|-------------------|
| Linear equations | `2x + 3 = 11` | âœ… Exact | âœ… RÂ²=0.711 |
| Quadratic equations | `xÂ² - 5x + 6 = 0` | âœ… Exact | âœ… RÂ²=0.999 |
| Systems (2x2) | `x + y = 5, x - y = 1` | âœ… Exact | ðŸ”„ Planned |
| Linear inequalities | `2x + 3 > 7` | âœ… Exact | âœ… RÂ²=0.996 |
| Quadratic inequalities | `xÂ² - 4 > 0` | âœ… Exact | ðŸ”„ Planned |
| Derivatives | `d/dx(xÂ³ + sin(x))` | âœ… Symbolic | â€” |

## Validation Scripts

Run the form invariance test to verify geometric understanding:

```bash
python geometry_vs_extraction_test.py
```

This proves the network learned mathematics, not pattern matching.

## The Thesis

> **Mathematical exactness emerges from geometric structure.**

Traditional neural networks treat math as string manipulation. This project demonstrates that:

1. Mathematical equations can be embedded in hyperbolic space
2. The embedding preserves mathematical meaning across syntactic variation
3. Solutions can be recovered directly from the geometric representation

The geometry IS the mathematics.

## Requirements

- Python 3.8+
- PyTorch 1.9+
- NumPy
- scikit-learn (for evaluation)
- matplotlib (for visualization)

## Citation

If you use this work, please cite:

```
@software{isomorphic_math_engine,
  author = {Big J},
  title = {Isomorphic Math Engine: Geometric Embeddings for Mathematical Equations},
  year = {2024},
  url = {https://github.com/xfdbv99pqh-jpg/isoengine}
}
```

## License

MIT License

## Acknowledgments

Developed through extensive experimentation exploring connections between:
- Hyperbolic geometry and hierarchical structure
- Contrastive learning and mathematical equivalence
- Transformer architectures and symbolic reasoning

Special thanks to Claude for pair programming and hypothesis testing.

---

**The embedding doesn't encode what the equation looks like â€” it encodes what the equation means.**
